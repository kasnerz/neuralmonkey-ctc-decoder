[vars]
prefix="/lnet/spec/work/people/kasner/neuralmonkey-ctc-decoder"
exp_prefix="{prefix}/experiments/backtranslations"
runners_batch_size=40
vocab_size=50000
trainer_batches_per_update=10
src="en"
tgt="de"
suffix="-kombo-complex-b10-v{vocab_size}"
use_pos=False
langpair="{src}{tgt}"
data_prefix="/lnet/troja/projects/mt-with-ctc-and-lm/backtranslations/{langpair}"
src_train_name="mixed.src"
tgt_train_name="mixed.tgt"
validation_data_prefix="{prefix}/data/{langpair}"
src_val_name="validation.{src}.{vocab_size}.spm"
tgt_val_name="validation.{tgt}.{vocab_size}.spm"

[main]
name="EN -> DE, SAN > split states > CTC"
tf_manager=<tf_manager>
output="{exp_prefix}/{langpair}-san_ctc{suffix}"
epochs=10
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner>]
postprocess=None
evaluation=[("target", evaluators.bleu.BLEU)]
logging_period="10m"
validation_period="2h"
runners_batch_size=$runners_batch_size
random_seed=1234
overwrite_output_dir=False
batching_scheme=<batch_scheme>

[batch_scheme]
class=dataset.BatchingScheme
batch_size=400
token_level_batching=True

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=12
num_sessions=1
save_n_best=5

[train_data]
class=dataset.load
series=["source", "target"]
data=["{data_prefix}/{src_train_name}","{data_prefix}/{tgt_train_name}"]
buffer_size=500000

[val_data]
class=dataset.load
series=["source", "target"]
data=["{validation_data_prefix}/{src_val_name}","{validation_data_prefix}/{tgt_val_name}"]

[vocabulary]
class=vocabulary.from_wordlist
path="/lnet/spec/work/people/kasner/spm/models/sp.{langpair}.{vocab_size}.vocab"
contains_frequencies=True
contains_header=False

[input_sequence]
class=model.sequence.EmbeddedSequence
vocabulary=<vocabulary>
data_id="source"
embedding_size=512
scale_embeddings_by_depth=True
max_length=64

[encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<input_sequence>
ff_hidden_size=4096
depth=6
n_heads=16
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9

[state_split]
class=model.sequence_split.SequenceSplitter
parent=<encoder>
projection_size=1536
factor=3

[second_encoder]
class=encoders.transformer.TransformerEncoder
input_sequence=<state_split>
ff_hidden_size=4096
depth=6
n_heads=16
dropout_keep_prob=0.9
attention_dropout_keep_prob=0.9
input_for_cross_attention=<encoder>
n_cross_att_heads=8
use_positional_encoding=$use_pos

[decoder]
#input_sequence=<input_sequence>
class=decoders.CTCDecoder
name="decoder"
max_length=64
encoder=<second_encoder>
data_id="target"
vocabulary=<vocabulary>

[obj]
class=trainers.cross_entropy_trainer.CostObjective
decoder=<decoder>

[trainer]
class=trainers.delayed_update_trainer.DelayedUpdateTrainer
clip_norm=1.0
batches_per_update=$trainer_batches_per_update
objectives=[<obj>]
optimizer=<adam>

[adam]
class=tf.contrib.opt.LazyAdamOptimizer
beta1=0.9
beta2=0.997
epsilon=1.0e-9
learning_rate=1.0e-4

[runner]
class=runners.plain_runner.PlainRunner
decoder=<decoder>
output_series="target"

